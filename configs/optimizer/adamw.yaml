optimizer:
  name: "AdamW"
  lr: 0.001
  betas: [0.9, 0.999]
  weight_decay: 0.01
  eps: 1e-8

  lr_scheduler:
    name: "cosine"
    warmup_iter: 1000
    min_lr: 1e-6

  mixed_precision:
    master_weights: true
    grad_scaler: 
      init_scale: 65536.0
      growth_interval: 2000